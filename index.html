<!DOCTYPE html>
<html>



  <head>

 <style>

hr {
        border: 0;
    height: 1px;
background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
}
.bottom-three {
     margin-bottom: 3cm;
}
</style>

  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TrailBlazer: Trajectory Control for Diffusion-Based Video Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/hohonu-vicml/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hohonu-vicml.github.io/DirectedDiffusion.Page">
            Directed Diffusion
          </a>
          <!-- <a class="navbar-item" href="https://nerfies.github.io"> -->
          <!--   Nerfies -->
          <!-- </a> -->
          <!-- <a class="navbar-item" href="https://latentfusion.github.io"> -->
          <!--   LatentFusion -->
          <!-- </a> -->
          <!-- <a class="navbar-item" href="https://photoshape.github.io"> -->
          <!--   PhotoShape -->
          <!-- </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TrailBlazer: Trajectory Control for Diffusion-Based Video Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/kurt-ma/">Wan-Duo Kurt Ma</a><sup>1</sup>,&nbsp&nbsp</span>
            <span class="author-block">
              <a href="http://www.scribblethink.org/">J. P. Lewis</a><sup>2</sup>,&nbsp&nbsp</span>
            <span class="author-block">
              <a href="https://people.wgtn.ac.nz/bastiaan.kleijn">W. Bastiaan Kleijn</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Victoria University of Wellington,&nbsp&nbsp</span>
            <span class="author-block"><sup>2</sup>NVIDIA Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2401.00896.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper(NEW:v2)</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2401.00896"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv(NEW:v2)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=kEN-32wN-xQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Project</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=P-PSkS7sNco"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Result</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hohonu-vicml/Trailblazer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/hohonu-vicml/Trailblazer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="far fa-images"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
             <!-- <\!-- dataset Link. -\-> -->
            <!--   <span class="link-block"> -->
            <!--     <a href="https://github.com/google/nerfies/releases/tag/0.1" -->
            <!--        class="external-link button is-normal is-rounded is-dark"> -->
            <!--       <span class="icon"> -->
            <!--           <i class="far fa-images"></i> -->
            <!--       </span> -->
            <!--       <span>Data</span> -->
            <!--       </a> -->
            <!-- </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/teaser.mov"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">TrailBlazer</span> features text-to-video diffusion video editing using a pre-trained model without further model training, finetuning, or online optimization. It supports various user experiences as depicted.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small"> -->
<!--   <div class="hero-body"> -->
<!--     <div class="container"> -->
<!--       <div id="results-carousel" class="carousel results-carousel"> -->
<!--         <div class="item item-steve"> -->
<!--           <video poster="" id="steve" autoplay controls muted loop playsinline height="100%"> -->
<!--             <source src="./static/videos/steve.mp4" -->
<!--                     type="video/mp4"> -->
<!--           </video> -->
<!--         </div> -->
<!--         <div class="item item-chair-tp"> -->
<!--           <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%"> -->
<!--             <source src="./static/videos/chair-tp.mp4" -->
<!--                     type="video/mp4"> -->
<!--           </video> -->
<!--         </div> -->
<!--         <div class="item item-shiba"> -->
<!--           <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%"> -->
<!--             <source src="./static/videos/shiba.mp4" -->
<!--                     type="video/mp4"> -->
<!--           </video> -->
<!--         </div> -->
<!--         <div class="item item-fullbody"> -->
<!--           <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%"> -->
<!--             <source src="./static/videos/fullbody.mp4" -->
<!--                     type="video/mp4"> -->
<!--           </video> -->
<!--         </div> -->
<!--         <div class="item item-blueshirt"> -->
<!--           <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%"> -->
<!--             <source src="./static/videos/blueshirt.mp4" -->
<!--                     type="video/mp4"> -->
<!--           </video> -->
<!--         </div> -->
<!--         <div class="item item-mask"> -->
<!--           <video poster="" id="mask" autoplay controls muted loop playsinline height="100%"> -->
<!--             <source src="./static/videos/mask.mp4" -->
<!--                     type="video/mp4"> -->
<!--           </video> -->
<!--         </div> -->
<!--         <div class="item item-coffee"> -->
<!--           <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%"> -->
<!--             <source src="./static/videos/coffee.mp4" -->
<!--                     type="video/mp4"> -->
<!--           </video> -->
<!--         </div> -->
<!--         <div class="item item-toby"> -->
<!--           <video poster="" id="toby" autoplay controls muted loop playsinline height="100%"> -->
<!--             <source src="./static/videos/toby2.mp4" -->
<!--                     type="video/mp4"> -->
<!--           </video> -->
<!--         </div> -->
<!--       </div> -->
<!--     </div> -->
<!--   </div> -->
<!-- </section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Large text-to-video (T2V) models such as Sora have the potential to revolutionize visual effects and the 
          creation of some types of movies. Current T2V models require tedious trial-and-error experimentation to 
          achieve desired results, however. This motivates the search for methods to directly control desired attributes. 
          In this work, we take a step toward this goal, introducing a method for high-level, temporally-coherent 
          control over the basic trajectories and appearance of objects.  Our algorithm, <span class="dnerf">TrailBlazer</span>, allows 
          the general positions and (optionally) appearance of objects to controlled simply by keyframing approximate 
          bounding boxes and (optionally) their corresponding prompts. </p>
          
          <p> Importantly, our method does not require a pre-existing control video signal that already contains an accurate 
          outline of the desired motion, yet the synthesized motion is surprisingly natural with emergent effects including 
          perspective and movement toward the virtual camera as the box size increases. The method is efficient, making use of 
          a pre-trained T2V model and requiring no training or fine-tuning, with negligible additional computation. Specifically, 
          the bounding box controls are used as soft masks to guide manipulation of the self-attention and cross-attention 
          modules in the video model. While our visual results are limited by those of the underlying model, the algorithm may 
          generalize to future models that use standard self- and cross-attention components.</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
  <!--   <div class="columns is-centered has-text-centered"> -->
  <!--     <div class="column is-four-fifths"> -->
  <!--       <h2 class="title is-3">Project Video</h2> -->
  <!--       <div class="publication-video"> -->
  <!--         <iframe src="https://www.youtube.com/embed/VAl7q9dHGwY" -->
  <!--                 frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
  <!--       </div> -->
  <!--     </div> -->
  <!--   </div> -->
  <!--   <\!-- Paper video. -\-> -->
  <!--   <div class="columns is-centered has-text-centered"> -->
  <!--     <div class="column is-four-fifths"> -->
  <!--       <h2 class="title is-3">Result Video</h2> -->
  <!--       <div class="publication-video"> -->
  <!--         <iframe src="https://www.youtube.com/embed/VAl7q9dHGwY" -->
  <!--                 frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
  <!--       </div> -->
  <!--     </div> -->
  <!--   </div> -->
  <!-- </div> -->


</section>



<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- <\!-- Visual Effects. -\-> -->
      <!-- <div class="column"> -->
      <!--   <div class="content"> -->
      <!--     <h2 class="title is-3">Visual Effects</h2> -->
      <!--     <p> -->
      <!--       Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect -->
      <!--       would be impossible without nerfies since it would require going through a wall. -->
      <!--     </p> -->
      <!--     <video id="dollyzoom" autoplay controls muted loop playsinline height="100%"> -->
      <!--       <source src="./static/videos/dollyzoom-stacked.mp4" -->
      <!--               type="video/mp4"> -->
      <!--     </video> -->
      <!--   </div> -->
      <!-- </div> -->
      <!-- <\!--/ Visual Effects. -\-> -->

      <!-- <\!-- Matting. -\-> -->
      <!-- <div class="column"> -->
      <!--   <h2 class="title is-3">Matting</h2> -->
      <!--   <div class="columns is-centered"> -->
      <!--     <div class="column content"> -->
      <!--       <p> -->
      <!--         As a byproduct of our method, we can also solve the matting problem by ignoring -->
      <!--         samples that fall outside of a bounding box during rendering. -->
      <!--       </p> -->
      <!--       <video id="matting-video" controls playsinline height="100%"> -->
      <!--         <source src="./static/videos/matting.mp4" -->
      <!--                 type="video/mp4"> -->
      <!--       </video> -->
      <!--     </div> -->
      <!--   </div> -->
      <!-- </div> -->


    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Core method -->
        <h2 class="title is-3">Update</h2>
        <div class="content has-text-justified">
          <ul>
            <li><b>[2024/Apr/08]</b> <span class="dnerf">TrailBlazer</span> has new v2 preprint on ArXiv </li>
            <li><b>[2024/Mar/22]</b> <span class="dnerf">TrailBlazer</span> is about to update both the ArXiv paper and its codebase in the upcoming week. </li>
            <li><b>[2024/Feb/06]</b> We now have Gradio web app at Huggingface Space! </li>
            <li><b>[2024/Jan/03]</b>  <span class="dnerf">TrailBlazer</span>  v1 is released on ArXiv. </li>
            <li><b>[2023/Dec/31]</b>  <span class="dnerf">TrailBlazer</span>  is submitted on ArXiv.. </li>
          </ul>
        </div>
        <br/><br/>
        <!-- Core method -->
        <h2 class="title is-3">Core Method</h2>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">TrailBlazer</span> highlights the central
            components of spatial cross-attention editing (left, in the
            almond-colored section) and temporal cross-frame attention editing
            (right, in the blue section). This operation is exclusively applied
            during the denoising process in the early stage. The objective is to
            alter the attention map within a user-specified bounding box (bbox).
            For more in-depth information, please consult our main text.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD.0003.mov"
                type="video/mp4">
          </video>
        </div>

        <hr class="bottom-three">
        <!-- Compositing -->
        <h2 class="title is-3">Scene compositing</h2>
        <!-- <h2 class="title is-4">Method</h2> -->
        <div class="content has-text-justified">
          <p>
            Scene compositing allows the motion of several subjects to be
            simultaneously controlled.This algorithm first computes the initial
            denoising steps of each subject individually. The first figure below
            shows the synthesis of “a white cat” and “a yellow dog”
            individually, serving as a sanity check for the quality of the
            subjects.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD.0004.mov"
                type="video/mp4">
      </video>
        </div>

        <!-- <h2 class="title is-4">Result</h2> -->
        <div class="content has-text-justified">
          <p>
            Then, these per-subject intermediate results are composited and
            processed by a global denoising under the control of a complete
            prompt (“a white cat and a yellow dog…”) that includes a description
            of the environment (e.g., “...on the moon”). Note that interactions
            between the background and subjects appear plausible, as seen in the
            consistent shadows across all samples.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD.0005.mov"
                type="video/mp4">
          </video>
        </div>
        <br/><br/>

<!--  _  __           __                     _              -->
<!-- | |/ /___ _   _ / _|_ __ __ _ _ __ ___ (_)_ __   __ _  -->
<!-- | ' // _ \ | | | |_| '__/ _` | '_ ` _ \| | '_ \ / _` | -->
<!-- | . \  __/ |_| |  _| | | (_| | | | | | | | | | | (_| | -->
<!-- |_|\_\___|\__, |_| |_|  \__,_|_| |_| |_|_|_| |_|\__, | -->
<!--           |___/                                 |___/  -->
        <hr class="bottom-three">
        <!-- Pipelin -->
        <h2 class="title is-3">Keyframing</h2>
        <div class="content has-text-justified">
          <p>
            The bounding boxes and prompts can be animated via keyframes,
            enabling users to alter the trajectory and coarse behavior of the
            subject along the timeline. The resulting subject(s) fit seamlessly
            in the specified environment, providing a viable pipeline for video
            storytelling by casual users.
          </p>
          <p> Please be aware that the annotated bounding boxes in all
          experiments below have been manually animated to enhance the viewing
          experience. </p>
        </div>

        <div class="content has-text-centered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD.0007.mov"
                type="video/mp4">
      </video>
        </div>
        <!-- Keyframe result -->
         <br/><br/><br/>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">TrailBlazer</span> features a novel way to guide
            the synthesized subject through bbox keyframing. For instance, the
            user can animate the fish swimming towards the camera and then goes
            away. Or, the user can control the cat running speed through
            keyframing.
          </p>
        </div>
        <div class="content has-text-centered">
        <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/Keyframe-bbox.mov"
                type="video/mp4">
        </video>
        <br/><br/><br/>
        <div class="content has-text-justified">
         <p>
            In addition, <span class="dnerf">TrailBlazer</span> demonstrates
            subject morphing via prompt keyframing. Examples include
            transformations from a cat to a dog, cat to fish, parrot to penguin,
            and tiger to elephant, as depicted below.
         </p>
         </div>
        <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/Keyframe-subject.mov"
                type="video/mp4">
        </video>
        </div>
        <br/><br/>
        <!-- Keyframe result -->

<!--  _____      _                            ____                _  -->
<!-- | ____|_  _| |_ _ __ ___ _ __ ___   ___ / ___|___  _ __   __| | -->
<!-- |  _| \ \/ / __| '__/ _ \ '_ ` _ \ / _ \ |   / _ \| '_ \ / _` | -->
<!-- | |___ >  <| |_| | |  __/ | | | | |  __/ |__| (_) | | | | (_| | -->
<!-- |_____/_/\_\\__|_|  \___|_| |_| |_|\___|\____\___/|_| |_|\__,_| -->

        <hr class="bottom-three">
        <!-- Pipelin -->
        <h2 class="title is-3">Extreme Conditions with Peekaboo</h2>
        <div class="content has-text-justified">
          <p>
            Here, we present a comparative analysis between <span class="dnerf">TrailBlazer</span> and the previous
            approach, <a href="https://jinga-lala.github.io/projects/Peekaboo/">Peekaboo</a>, under controlled conditions. In particular, we
            examine the manipulation of bounding box (bbox) keyframing in
            extreme scenarios, including rapid changes in bbox size, irregular
            bbox trajectories, swift motion determined by the middle keyframe,
            the number of keyframes required to move the subject to its opposite
            side, and static small bbox.
          </p>
          <p>Please see our paper for more detail and the full metric (e.g., FID, FVD, mIoU,..) comparisons and reasonings.</p>
        </div>
        <div class="content has-text-centered">
           <br/><br/>
         <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD-Comp.0001.mov"
                type="video/mp4">
         </video>
          <p> E.g., Our representation of an elephant maintains a stationary
            position for initial 75% of video before initiating movement. </p>
          <p>Prompt: An elephant walking on the moon </p>
          <br/><br/>
         <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD-Comp.0002.mov"
                type="video/mp4">
         </video>
         <p> E.g., The whale gracefully descends into the ocean during the latter part of its jumping motion.</p>
         <p>Prompt: a photorealistic whale jumping out of water while smoking a cigar</p>
         <br/><br/>
         <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD-Comp.0003.mov"
                type="video/mp4">
         </video>
         <p>E.g., The horse accurately follows a zigzag path, simulating a galloping motion.</p>
         <p>Prompt: A horse galloping fast on a street</p>
         <br/><br/>
         <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD-Comp.0004.mov"
                type="video/mp4">
         </video>
         <p> E.g., Remarkably, the dog seamlessly follows a large number of
         keyframes (8 keyframes) within a 24-frame video clip, covering the
         distance from one boundary to the opposite in approximately 2 time
         frames. </p>
           <p>Prompt: A dog is running on the grass</p>
           <br/><br/>
         <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/dd/TDD-Comp.0005.mov"
                type="video/mp4">
         </video>
         <p> E.g., The clownfish fits into a tiny bbox </p>
         <p>Prompt: A clownfish swimming in a coral ree</p>
        </div>
        <br/><br/>






        <hr class="bottom-three">
        <h2 class="title is-3">Limitations</h2>
        <div class="content has-text-justified">
          <p>
           <span class="dnerf">TrailBlazer</span> inherits the limitations of the underlying pre-trained model (ZeroScope). These include animals with an incorrect number of limbs and other issues common to a number of diffusion-based T2I and T2V methods.
          </p>
        </div>


        <!-- Pipelin -->
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            Our contributions are listed below:
          </p>
          <ul>
            <li>
              <strong>Novelty</strong>:
              We introduce a novel approach, <span class="dnerf">TrailBlazer</span>, employing
              high-level bounding boxes to guide the
              subject in diffusion-based video synthesis. This approach is suitable for casual
              users, as it avoids the need to record or draw a frame-by-frame positioning control
              signal. In contrast, the low-level guidance signals (detailed masks, edge maps)
              used by some other approaches have two disadvantages: it is difficult for non-artists
              to draw these shapes, and processing existing videos to obtain these signals limits
              the available motion to copies of existing sources.
            </li>
            <ul>
              </ul>
            <li>
              <strong>Trajectory control</strong>:
              <span class="dnerf">TrailBlazer</span> enables users to position the subject by keyframing its bounding box.
              The size of the bbox can be similarly controlled, thereby producing directional motion and perspective effects.
              Finally, users can also keyframe the text prompt to influence the behavior of the subject in the synthesized video.
            </li>
            <ul>
              </ul>
            <li>
              <strong>Simplicity</strong>:
              <span class="dnerf">TrailBlazer</span> operates by directly editing the spatial and temporal attention in the
              pre-trained denoising UNet. It requiring no training or optimization, and the core algorithm can be implemented
              in less than 200 lines of code.
            </li>
          </ul>
        </div>
        <!-- Keyframe result -->


        <br/><br/><hr class="bottom-three">
        <!-- Pipelin -->
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">TrailBlazer</span> will be further enhanced to
            improve the quality and usability. If you find our work interesting,
            please cite our article.
          </p>
        </div>
        <!-- Keyframe result -->

      </div>
      </div>
    </div>
  </div>
</section>







<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{ma2023trailblazer,
            title={TrailBlazer: Trajectory Control for Diffusion-Based Video Generation},
            author={Wan-Duo Kurt Ma and J. P. Lewis and W. Bastiaan Kleijn},
            year={2023},
            eprint={2401.00896},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
      }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/hohonu-vicml" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website utilizes the page template developed by
            the <a href="https://nerfies.github.io">Nerfies</a> team. The
            authors extend their appreciation for their diligent efforts in
            creating this highly flexible and high-quality work.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
